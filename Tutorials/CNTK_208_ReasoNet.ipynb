{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 208: ReasoNet for Machine Comprehension\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "This hands-on tutorial will take you through how to implement [ReasoNet](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf) in the Microsoft Cognitive Toolkit. Machine comprehension task try to find out the answer for a question given a paragraph of text. \n",
    "In this tutorial, we will use [CNN data](https://github.com/deepmind/rc-data) as an example. The data is consist of tuples (q,d,a,A). Here q is the query, d is the document, a is candidate list and A is the true answer. \n",
    "\n",
    "### Model Structure\n",
    "\n",
    "![](ReasoNet/components.png) \n",
    "![](ReasoNet/reasonet.png) \n",
    "\n",
    "## Data preparing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "The data can be downloaded via (https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM) or (https://github.com/deepmind/rc-data)\n",
    "The downloaded data is packaged as a gz file and to feed to CNTK it needs to be reformated. After unpacking the file, we will get three folders (e.g. training, test, validation), each contains a lot of files where each file is consist of a paragraph of text, a question, the answer to the questions and a list of entities. First we need to merge each folder of files into a single file with following script,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "def merge_files(folder, target):\n",
    "  if os.path.exists(target):\n",
    "    return\n",
    "  count = 0\n",
    "  all_files = os.listdir(folder)\n",
    "  print(\"Start to merge {0} files under folder {1} as {2}\".format(len(all_files), folder, target))\n",
    "  for f in all_files:\n",
    "    txt=os.path.join(folder, f)\n",
    "    if os.path.isfile(txt):\n",
    "      with open(txt) as sample:\n",
    "        content = sample.readlines()\n",
    "        context = content[2].strip()\n",
    "        query = content[4].strip()\n",
    "        answer = content[6].strip()\n",
    "        entities = []\n",
    "        for k in range(8, len(content)):\n",
    "          entities += [ content[k].strip() ]\n",
    "        with open(target, 'a') as output:\n",
    "          output.write(\"{0}\\t{1}\\t{2}\\t{3}\\n\".format(query, answer, context, \"\\t\".join(entities)))\n",
    "    count+=1\n",
    "    if count%1000==0:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "  print()\n",
    "  print(\"Finished to merge {0}\".format(target))\n",
    "\n",
    "def download_cnn(target=\".\"):\n",
    "  if os.path.exists(os.path.join(target, \"cnn\")):\n",
    "    shutil.rmtree(os.path.join(target, \"cnn\"))\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "  url=\"https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM\"\n",
    "  print(\"Start to download CNN data from {0} to {1}\".format(url, target))\n",
    "  pre_request = requests.get(url)\n",
    "  confirm_match = re.search(r\"confirm=(.{4})\", pre_request.content.decode(\"utf-8\"))\n",
    "  confirm_url = url + \"&confirm=\" + confirm_match.group(1)\n",
    "  download_request = requests.get(confirm_url, cookies=pre_request.cookies)\n",
    "  tar = tarfile.open(mode=\"r:gz\", fileobj=io.BytesIO(download_request.content))\n",
    "  tar.extractall(target)\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "\n",
    "def file_exists(src):\n",
    "  return (os.path.isfile(src) and os.path.exists(src))\n",
    "\n",
    "data_path = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "raw_train_data=os.path.join(data_path, \"training.txt\")\n",
    "raw_test_data=os.path.join(data_path, \"test.txt\")\n",
    "raw_validation_data=os.path.join(data_path, \"validation.txt\")\n",
    "if not (file_exists(raw_train_data) and file_exists(raw_test_data) and file_exists(raw_validation_data)):\n",
    "  download_cnn(data_path)\n",
    "\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/training\"), raw_train_data)\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/test\"), raw_test_data)\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/validation\"), raw_validation_data)\n",
    "print(\"All necessary data are downloaded to {0}\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to CNTK Text Format\n",
    "\n",
    "CNTK consumes a special text format for training data, we need to convert the downloaded data fiels into [CNTK text format](https://github.com/Microsoft/CNTK/wiki/BrainScript-CNTKTextFormat-Reader). Here is the script to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "class WordFreq:\n",
    "  def __init__(self, word, id, freq):\n",
    "    self.word = word\n",
    "    self.id = id\n",
    "    self.freq = freq\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"Build word vocabulary with frequency\"\"\"\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.size = 0\n",
    "    self.__dict = {}\n",
    "    self.__has_index = False\n",
    "\n",
    "  def push(self, word):\n",
    "    if word in self.__dict:\n",
    "      self.__dict[word].freq += 1\n",
    "    else:\n",
    "      self.__dict[word] = WordFreq(word, len(self.__dict), 1)\n",
    "\n",
    "  def build_index(self, max_size):\n",
    "    def word_cmp(x, y):\n",
    "      if x.freq == y.freq :\n",
    "        return (x.word > y.word) - (x.word < y.word)\n",
    "      else:\n",
    "        return x.freq - y.freq\n",
    "\n",
    "    items = sorted(self.__dict.values(), key=functools.cmp_to_key(word_cmp), reverse=True)\n",
    "    if len(items)>max_size:\n",
    "      del items[max_size:]\n",
    "    self.size=len(items)\n",
    "    self.__dict.clear()\n",
    "    for it in items:\n",
    "      it.id = len(self.__dict)\n",
    "      self.__dict[it.word] = it\n",
    "    self.__has_index = True\n",
    "\n",
    "  def save(self, dst):\n",
    "    if not self.__has_index:\n",
    "      self.build_index(sys.maxsize)\n",
    "    if self.name != None:\n",
    "      dst.write(\"{0}\\t{1}\\n\".format(self.name, self.size))\n",
    "    for it in sorted(self.__dict.values(), key=lambda it:it.id):\n",
    "      dst.write(\"{0}\\t{1}\\t{2}\\n\".format(it.word, it.id, it.freq))\n",
    "\n",
    "  def load(self, src):\n",
    "    line = src.readline()\n",
    "    if line == \"\":\n",
    "      return\n",
    "    line = line.rstrip('\\n')\n",
    "    head = line.split()\n",
    "    max_size = sys.maxsize\n",
    "    if len(head) == 2:\n",
    "      self.name = head[0]\n",
    "      max_size = int(head[1])\n",
    "    cnt = 0\n",
    "    while cnt < max_size:\n",
    "      line = src.readline()\n",
    "      if line == \"\":\n",
    "        break\n",
    "      line = line.rstrip('\\n')\n",
    "      items = line.split()\n",
    "      self.__dict[items[0]] = WordFreq(items[0], int(items[1]), int(items[2]))\n",
    "      cnt += 1\n",
    "    self.size = len(self.__dict)\n",
    "    self.__has_index = True\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    if key in self.__dict:\n",
    "      return self.__dict[key]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def values(self):\n",
    "    return self.__dict.values()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __contains__(self, q):\n",
    "    return q in self.__dict\n",
    "\n",
    "  @staticmethod\n",
    "  def is_cnn_entity(word):\n",
    "    return word.startswith('@entity') or word.startswith('@placeholder')\n",
    "\n",
    "  @staticmethod\n",
    "  def load_vocab(vocab_src):\n",
    "    \"\"\"\n",
    "    Loa vocabulary from file.\n",
    "\n",
    "    Args:\n",
    "      vocab_src (`str`): the file stored with the vocabulary data\n",
    "      \n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    with open(vocab_src, 'r', encoding='utf-8') as src:\n",
    "      entity_vocab.load(src)\n",
    "      word_vocab.load(src)\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def build_vocab(input_src, vocab_dst, max_size=50000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from raw corpus file.\n",
    "\n",
    "    Args:\n",
    "      input_src (`str`): the path of the corpus file\n",
    "      vocab_dst (`str`): the path of the vocabulary file to save the built vocabulary\n",
    "      max_size (`int`): the maxium size of the word vocabulary\n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    # Leave the first as Unknown\n",
    "    max_size -= 1\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    linenum = 0\n",
    "    print(\"Start build vocabulary from {0} with maxium words {1}. Saved to {2}\".format(input_src, max_size, vocab_dst))\n",
    "    with open(input_src, 'r', encoding='utf-8') as src:\n",
    "      all_lines = src.readlines()\n",
    "      print(\"Total lines to process: {0}\".format(len(all_lines)))\n",
    "      for line in all_lines:\n",
    "        line = line.strip('\\n')\n",
    "        ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "        for q in query_words:\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "          #if q.startswith('@'):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        for q in context_words:\n",
    "          #if q.startswith('@'):\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        linenum += 1\n",
    "        if linenum%1000==0:\n",
    "          sys.stdout.write(\".\")\n",
    "          sys.stdout.flush()\n",
    "    print()\n",
    "    entity_vocab.build_index(max_size)\n",
    "    word_vocab.build_index(max_size)\n",
    "    with open(vocab_dst, 'w', encoding='utf-8') as dst:\n",
    "      entity_vocab.save(dst)\n",
    "      word_vocab.save(dst)\n",
    "    print(\"Finished to generate vocabulary from: {0}\".format(input_src))\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def parse_corpus_line(line):\n",
    "    \"\"\"\n",
    "    Parse bing corpus line to answer, query and context.\n",
    "\n",
    "    Args:\n",
    "      line (`str`): A line of text of bing corpus\n",
    "    Returns:\n",
    "      :`str`: Answer word\n",
    "      :`str[]`: Array of query words\n",
    "      :`str[]`: Array of context/passage words\n",
    "\n",
    "    \"\"\"\n",
    "    data = line.split('\\t')\n",
    "    query = data[0]\n",
    "    answer = data[1]\n",
    "    context = data[2]\n",
    "    query_words = query.split()\n",
    "    context_words = context.split()\n",
    "    return answer, query_words, context_words\n",
    "\n",
    "  def build_corpus(entities, words, corpus, output, max_seq_len=100000):\n",
    "    \"\"\"\n",
    "    Build featurized corpus and store it in CNTK Text Format.\n",
    "\n",
    "    Args:\n",
    "      entities (class:`Vocabulary`): The entities vocabulary\n",
    "      words (class:`Vocabulary`): The words vocabulary\n",
    "      corpus (`str`): The file path of the raw corpus\n",
    "      output (`str`): The file path to store the featurized corpus data file\n",
    "    \"\"\"\n",
    "    seq_id = 0\n",
    "    print(\"Start to build CTF data from: {0}\".format(corpus))\n",
    "    with open(corpus, 'r', encoding = 'utf-8') as corp:\n",
    "      with open(output, 'w', encoding = 'utf-8') as outf:\n",
    "        all_lines = corp.readlines()\n",
    "        print(\"Total lines to prcess: {0}\".format(len(all_lines)))\n",
    "        for line in all_lines:\n",
    "          line = line.strip('\\n')\n",
    "          ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "          ans_item = entities[ans]\n",
    "          query_ids = []\n",
    "          context_ids = []\n",
    "          is_entity = []\n",
    "          entity_ids = []\n",
    "          labels = []\n",
    "          pos = 0\n",
    "          answer_idx = None\n",
    "          for q in context_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              context_ids += [ item.id + 1 ]\n",
    "              entity_ids += [ item.id + 1 ]\n",
    "              is_entity += [1]\n",
    "              if ans_item.id == item.id:\n",
    "                labels += [1]\n",
    "                answer_idx = pos\n",
    "              else:\n",
    "                labels += [0]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              context_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "              is_entity += [0]\n",
    "              labels += [0]\n",
    "            pos += 1\n",
    "            if (pos >= max_seq_len):\n",
    "              break\n",
    "          if answer_idx is None:\n",
    "            continue\n",
    "          for q in query_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              query_ids += [ item.id + 1 ]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              query_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "          #Write featurized ids\n",
    "          outf.write(\"{0}\".format(seq_id))\n",
    "          for i in range(max(len(context_ids), len(query_ids))):\n",
    "            if i < len(query_ids):\n",
    "              outf.write(\" |Q {0}:1\".format(query_ids[i]))\n",
    "            if i < len(context_ids):\n",
    "              outf.write(\" |C {0}:1\".format(context_ids[i]))\n",
    "              outf.write(\" |E {0}\".format(is_entity[i]))\n",
    "              outf.write(\" |L {0}\".format(labels[i]))\n",
    "            if i < len(entity_ids):\n",
    "              outf.write(\" |EID {0}:1\".format(entity_ids[i]))\n",
    "            outf.write(\"\\n\")\n",
    "          seq_id += 1\n",
    "          if seq_id%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    print(\"Finished to build corpus from {0}\".format(corpus))\n",
    "  \n",
    "vocab_path=os.path.join(data_path, \"cnn.vocab\")\n",
    "train_ctf=os.path.join(data_path, \"training.ctf\")\n",
    "test_ctf=os.path.join(data_path, \"test.ctf\")\n",
    "validation_ctf=os.path.join(data_path, \"validation.ctf\")\n",
    "vocab_size=101000\n",
    "if not (file_exists(train_ctf) and file_exists(test_ctf) and file_exists(validation_ctf)):\n",
    "  entity_vocab, word_vocab = Vocabulary.build_vocab(raw_train_data, vocab_path, vocab_size)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_train_data, train_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_test_data, test_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_validation_data, validation_ctf)\n",
    "print(\"Training data conversion finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reader\n",
    "The data is stored in CNTK Text Format and we need to create a reader to consume the data. There are 5 columns/streams in the data file, e.g. context, query, entity indication, label, entity ids. Here is an example,\n",
    "\n",
    "0 |Q 586:1 |C 626:1 |E 0 |L 0 |EID 3:1\n",
    "\n",
    " |Q 12:1 |C 3:1 |E 1 |L 0 |EID 5:1\n",
    " \n",
    " |Q 2758:1 |C 625:1 |E 0 |L 0 |EID 4:1\n",
    " \n",
    " |Q 603:1 |C 1268:1 |E 0 |L 0 |EID 8:1\n",
    " \n",
    " |Q 933:1 |C 1516:1 |E 0 |L 0 |EID 10:1\n",
    " \n",
    " |Q 594:1 |C 757:1 |E 0 |L 0 |EID 13:1\n",
    " \n",
    " |Q 33:1 |C 586:1 |E 0 |L 0 |EID 14:1\n",
    " \n",
    " |Q 587:1 |C 4669:1 |E 0 |L 0 |EID 23:1\n",
    " \n",
    " |Q 10:1 |C 1712:1 |E 0 |L 0 |EID 10:1\n",
    " \n",
    " |Q 594:1 |C 591:1 |E 0 |L 0 |EID 10:1\n",
    "\n",
    "Here the first column is the sequence id, 0. The second is the features of Query, the third is the features of Context, the fourth is a boolean to indicate if that word in the Context is an entity, the fifth is the Label which indicate if that word in the context is the answer. The last is the ID of entities in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, DEFAULT_RANDOMIZATION_WINDOW\n",
    "import cntk.ops as ops\n",
    "from cntk.layers.blocks import _INFERRED, Parameter\n",
    "from cntk.internal import _as_tuple, sanitize_input\n",
    "import cntk.learner as learner\n",
    "\n",
    "def create_reader(path, vocab_dim, entity_dim, randomize, rand_size= DEFAULT_RANDOMIZATION_WINDOW, size=INFINITELY_REPEAT):\n",
    "  \"\"\"\n",
    "  Create data reader for the model\n",
    "  Args:\n",
    "    path: The data path\n",
    "    vocab_dim: The dimention of the vocabulary\n",
    "    entity_dim: The dimention of entities\n",
    "    randomize: Where to shuffle the data before feed into the trainer\n",
    "  \"\"\"\n",
    "  return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "    context  = StreamDef(field='C', shape=vocab_dim, is_sparse=True),\n",
    "    query    = StreamDef(field='Q', shape=vocab_dim, is_sparse=True),\n",
    "    entities  = StreamDef(field='E', shape=1, is_sparse=False),\n",
    "    label   = StreamDef(field='L', shape=1, is_sparse=False),\n",
    "    entity_ids   = StreamDef(field='EID', shape=entity_dim, is_sparse=True)\n",
    "    )), randomize=randomize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "We need some utils to be used in the model creation and training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "class logger:\n",
    "  __name=''\n",
    "  __logfile=''\n",
    "\n",
    "  @staticmethod\n",
    "  def init(name=''):\n",
    "    if not os.path.exists(\"model\"):\n",
    "      os.mkdir(\"model\")\n",
    "    if not os.path.exists(\"log\"):\n",
    "      os.mkdir(\"log\")\n",
    "    if name=='' or name is None:\n",
    "      logger.__name='train'\n",
    "    logger.__logfile = 'log/{}_{}.log'.format(logger.__name, datetime.now().strftime(\"%m-%d_%H.%M.%S\"))\n",
    "    if os.path.exists(logger.__logfile):\n",
    "      os.remove(logger.__logfile)\n",
    "    print('Log with log file: {0}'.format(logger.__logfile))\n",
    "\n",
    "  @staticmethod\n",
    "  def log(message, toconsole=True):\n",
    "    if logger.__logfile == '' or logger.__logfile is None:\n",
    "      logger.init()\n",
    "    if toconsole:\n",
    "      print(message)\n",
    "    with open(logger.__logfile, 'a') as logf:\n",
    "      logf.write(\"{}| {}\\n\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n",
    "\n",
    "class uniform_initializer:\n",
    "  def __init__(self, scale=1, bias=0, seed=0):\n",
    "    self.seed = seed\n",
    "    self.scale = scale\n",
    "    self.bias = bias\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def reset(self):\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def next(self, size=None):\n",
    "    return np.random.uniform(0, 1, size)*self.scale + self.bias\n",
    "\n",
    "def create_random_matrix(rows, columns):\n",
    "  scale = math.sqrt(6/(rows+columns))*2\n",
    "  rand = uniform_initializer(scale, -scale/2)\n",
    "  embedding = [None]*rows\n",
    "  for i in range(rows):\n",
    "    embedding[i] = np.array(rand.next(columns), dtype=np.float32)\n",
    "  return np.ndarray((rows, columns), dtype=np.float32, buffer=np.array(embedding))\n",
    "\n",
    "def load_embedding(embedding_path, vocab_path, dim, init=None):\n",
    "  entity_vocab, word_vocab = Vocabulary.load_bingvocab(vocab_path)\n",
    "  vocab_dim = len(entity_vocab) + len(word_vocab) + 1\n",
    "  entity_size = len(entity_vocab)\n",
    "  item_embedding = [None]*vocab_dim\n",
    "  with open(embedding_path, 'r') as embedding:\n",
    "    for line in embedding.readlines():\n",
    "      line = line.strip('\\n')\n",
    "      item = line.split(' ')\n",
    "      if item[0] in word_vocab:\n",
    "        item_embedding[word_vocab[item[0]].id + entity_size + 1] = np.array(item[1:], dtype=\"|S\").astype(np.float32)\n",
    "  if init != None:\n",
    "    init.reset()\n",
    "\n",
    "  for i in range(vocab_dim):\n",
    "    if item_embedding[i] is None:\n",
    "      if init:\n",
    "        item_embedding[i] = np.array(init.next(dim), dtype=np.float32)\n",
    "      else:\n",
    "        item_embedding[i] = np.array([0]*dim, dtype=np.float32)\n",
    "  return np.ndarray((vocab_dim, dim), dtype=np.float32, buffer=np.array(item_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic components\n",
    "Here we provide some basic components that will be used in the model to simplify the model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers.blocks import Stabilizer, _initializer_for,  _INFERRED, Parameter, Placeholder\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error, sequence, reduce_sum, \\\n",
    "    parameter, times, element_times, past_value, plus, placeholder_variable, reshape, constant, sigmoid, convolution, tanh, times_transpose, greater, cosine_distance, element_divide, element_select, exp, future_value, past_value\n",
    "from cntk.internal import _as_tuple, sanitize_input\n",
    "from cntk.initializer import uniform, glorot_uniform\n",
    "\n",
    "def gru_cell(shape, init=glorot_uniform(), name=''): # (x, (h,c))\n",
    "  \"\"\" GRU cell function\n",
    "  \"\"\"\n",
    "  shape = _as_tuple(shape)\n",
    "\n",
    "  if len(shape) != 1 :\n",
    "    raise ValueError(\"gru_cell: shape must be vectors (rank-1 tensors)\")\n",
    "\n",
    "  # determine stacking dimensions\n",
    "  cell_shape_stacked = shape * 2  # patched dims with stack_axis duplicated 2 times\n",
    "\n",
    "  # parameters\n",
    "  Wz = Parameter(cell_shape_stacked, init = init, name='Wz')\n",
    "  Wr = Parameter(cell_shape_stacked, init = init, name='Wr')\n",
    "  Wh = Parameter(cell_shape_stacked, init = init, name='Wh')\n",
    "  Uz = Parameter(_INFERRED + shape, init = init, name = 'Uz')\n",
    "  Ur = Parameter(_INFERRED + shape, init = init, name = 'Ur')\n",
    "  Uh = Parameter(_INFERRED + shape, init = init, name = 'Uh')\n",
    "\n",
    "  def create_s_placeholder():\n",
    "    # we pass the known dimensions here, which makes dimension inference easier\n",
    "    return Placeholder(shape=shape, name='S') # (h, c)\n",
    "\n",
    "  # parameters to model function\n",
    "  x = Placeholder(name='gru_block_arg')\n",
    "  prev_status = create_s_placeholder()\n",
    "\n",
    "  # formula of model function\n",
    "  Sn_1 = prev_status\n",
    "\n",
    "  z = sigmoid(times(x, Uz, name='x*Uz') + times(Sn_1, Wz, name='Sprev*Wz'), name='z')\n",
    "  r = sigmoid(times(x, Ur, name='x*Ur') + times(Sn_1, Wr, name='Sprev*Wr'), name='r')\n",
    "  h = tanh(times(x, Uh, name='x*Uh') + times(element_times(Sn_1, r, name='Sprev*r'), Wh), name='h')\n",
    "  s = plus(element_times((1-z), h, name='(1-z)*h'), element_times(z, Sn_1, name='z*SPrev'), name=name)\n",
    "  apply_x_s = combine([s])\n",
    "  apply_x_s.create_placeholder = create_s_placeholder\n",
    "  return apply_x_s\n",
    "\n",
    "def seq_max(x, broadcast=True, name=''):\n",
    "  \"\"\"\n",
    "  Get the max value in the sequence values\n",
    "\n",
    "  Args:\n",
    "    x: input sequence\n",
    "    broadcast: if broadcast is True, the max value will be broadcast along with the input sequence,\n",
    "    else only a single value will be returned\n",
    "    name: the name of the operator\n",
    "  \"\"\"\n",
    "  m = placeholder_variable(shape=(1,), dynamic_axes = x.dynamic_axes, name='max')\n",
    "  o = element_select(greater(x, future_value(m)), x, future_value(m))\n",
    "  rlt = o.replace_placeholders({m:sanitize_input(o)})\n",
    "  if broadcast:\n",
    "    pv = placeholder_variable(shape=(1,), dynamic_axes = x.dynamic_axes, name='max_seq')\n",
    "    max_seq = element_select(sequence.is_first(x), sanitize_input(rlt), past_value(pv))\n",
    "    max_out = max_seq.replace_placeholders({pv:sanitize_input(max_seq)})\n",
    "  else:\n",
    "    max_out = sequence.first(rlt)\n",
    "  return sanitize_input(max_out)\n",
    "\n",
    "def seq_softmax(x, name = ''):\n",
    "  \"\"\"\n",
    "  Compute softmax along with a squence values\n",
    "  \"\"\"\n",
    "  x_exp = exp((x-seq_max(x))*10)\n",
    "  x_softmax = element_divide(x_exp, sequence.broadcast_as(sequence.reduce_sum(x_exp), x), name = name)\n",
    "  return x_softmax\n",
    "\n",
    "def cosine_similarity(src, tgt, name=''):\n",
    "  \"\"\"\n",
    "  Compute the cosine similarity of two squences.\n",
    "  Src is a sequence of length 1\n",
    "  Tag is a sequence of lenght >=1\n",
    "  \"\"\"\n",
    "  src_br = sequence.broadcast_as(src, tgt, name='src_broadcast')\n",
    "  sim = cosine_distance(src_br, tgt, name)\n",
    "  return sim\n",
    "\n",
    "def project_cosine_sim(att_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the project cosine similarity of two input sequences, where each of the input will be projected to a new dimention space (att_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  Wi = Parameter(_INFERRED + tuple((att_dim,)), init = init, name='Wi')\n",
    "  Wm = Parameter(_INFERRED + tuple((att_dim,)), init = init, name='Wm')\n",
    "  status = placeholder_variable(name='status')\n",
    "  memory = placeholder_variable(name='memory')\n",
    "  projected_status = times(status, Wi, name = 'projected_status')\n",
    "  projected_memory = times(memory, Wm, name = 'projected_memory')\n",
    "  sim = cosine_similarity(projected_status, projected_memory, name= name+ '_sim')\n",
    "  return seq_softmax(sim, name = name)\n",
    "\n",
    "def termination_gate(init = glorot_uniform(), name=''):\n",
    "  Wt = Parameter( _INFERRED + tuple((1,)), init = init, name='Wt')\n",
    "  status = placeholder_variable(name='status')\n",
    "  return sigmoid(times(status, Wt), name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model_params:\n",
    "  def __init__(self, vocab_dim, entity_dim, hidden_dim, embedding_dim=100, embedding_init=None, share_rnn_param=False, max_rl_steps=5, dropout_rate=None, init=glorot_uniform(), model_name='rsn'):\n",
    "    self.vocab_dim = vocab_dim\n",
    "    self.entity_dim = entity_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embedding_init = embedding_init\n",
    "    self.max_rl_steps = max_rl_steps\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.init = init\n",
    "    self.model_name = model_name\n",
    "    self.share_rnn_param = share_rnn_param\n",
    "    self.attention_dim = 384\n",
    "\n",
    "def bind_data(func, data):\n",
    "  \"\"\"\n",
    "  Bind data outputs to cntk function arguments based on the argument name\n",
    "  \"\"\"\n",
    "  bind = {}\n",
    "  for arg in func.arguments:\n",
    "    if arg.name == 'query':\n",
    "      bind[arg] = data.streams.query\n",
    "    if arg.name == 'context':\n",
    "      bind[arg] = data.streams.context\n",
    "    if arg.name == 'entity_ids_mask':\n",
    "      bind[arg] = data.streams.entities\n",
    "    if arg.name == 'labels':\n",
    "      bind[arg] = data.streams.label\n",
    "    if arg.name == 'entity_ids':\n",
    "      bind[arg] = data.streams.entity_ids\n",
    "  return bind\n",
    "\n",
    "def attention_model(context_memory, query_memory, init_status, hidden_dim, att_dim, max_steps = 5, init = glorot_uniform()):\n",
    "  \"\"\"\n",
    "  Create the attention model for reasonet\n",
    "  Args:\n",
    "    context_memory: Context memory\n",
    "    query_memory: Query memory\n",
    "    init_status: Intialize status\n",
    "    hidden_dim: The dimention of hidden state\n",
    "    att_dim: The dimention of attention\n",
    "    max_step: Maxuim number of step to revisit the context memory\n",
    "  \"\"\"\n",
    "  gru = gru_cell((hidden_dim*2, ), name='control_status')\n",
    "  status = init_status\n",
    "  output = [None]*max_steps*2\n",
    "  sum_prob = None\n",
    "  context_cos_sim = project_cosine_sim(att_dim, name='context_attention')\n",
    "  query_cos_sim = project_cosine_sim(att_dim, name='query_attention')\n",
    "  ans_cos_sim = project_cosine_sim(att_dim, name='candidate_attention')\n",
    "  stop_gate = termination_gate(name='terminate_prob')\n",
    "  prev_stop = 0\n",
    "  for step in range(max_steps):\n",
    "    context_attention_weight = context_cos_sim(status, context_memory)\n",
    "    query_attention_weight = query_cos_sim(status, query_memory)\n",
    "    context_attention = sequence.reduce_sum(times(context_attention_weight, context_memory), name='C-Att')\n",
    "    query_attention = sequence.reduce_sum(times(query_attention_weight, query_memory), name='Q-Att')\n",
    "    attention = ops.splice(query_attention, context_attention, name='att-sp')\n",
    "    status = gru(attention, status).output\n",
    "    termination_prob = stop_gate(status)\n",
    "    ans_attention = ans_cos_sim(status, context_memory)\n",
    "    output[step*2] = ans_attention\n",
    "    if step < max_steps -1:\n",
    "      stop_prob = prev_stop + ops.log(termination_prob, name='log_stop')\n",
    "    else:\n",
    "      stop_prob = prev_stop\n",
    "    output[step*2+1] = sequence.broadcast_as(ops.exp(stop_prob, name='exp_log_stop'), output[step*2], name='Stop_{0}'.format(step))\n",
    "    prev_stop += ops.log(1-termination_prob, name='log_non_stop')\n",
    "\n",
    "  final_ans = None\n",
    "  for step in range(max_steps):\n",
    "    if final_ans is None:\n",
    "      final_ans = output[step*2] * output[step*2+1]\n",
    "    else:\n",
    "      final_ans += output[step*2] * output[step*2+1]\n",
    "  combine_func = combine(output + [ final_ans ], name='Attention_func')\n",
    "  return combine_func\n",
    "\n",
    "def create_model(params : model_params):\n",
    "  \"\"\"\n",
    "  Create ReasoNet model\n",
    "  Args:\n",
    "    params (class:`model_params`): The parameters used to create the model\n",
    "  \"\"\"\n",
    "  logger.log(\"Create model: dropout_rate: {0}, init:{1}, embedding_init: {2}\".format(params.dropout_rate, params.init, params.embedding_init))\n",
    "  # Query and Doc/Context/Paragraph inputs to the model\n",
    "  batch_axis = Axis.default_batch_axis()\n",
    "  query_seq_axis = Axis('sourceAxis')\n",
    "  context_seq_axis = Axis('contextAxis')\n",
    "  query_dynamic_axes = [batch_axis, query_seq_axis]\n",
    "  query_sequence = input_variable(shape=(params.vocab_dim), is_sparse=True, dynamic_axes=query_dynamic_axes, name='query')\n",
    "  context_dynamic_axes = [batch_axis, context_seq_axis]\n",
    "  context_sequence = input_variable(shape=(params.vocab_dim), is_sparse=True, dynamic_axes=context_dynamic_axes, name='context')\n",
    "  entity_ids_mask = input_variable(shape=(1,), is_sparse=False, dynamic_axes=context_dynamic_axes, name='entity_ids_mask')\n",
    "  # embedding\n",
    "  if params.embedding_init is None:\n",
    "    embedding_init = create_random_matrix(params.vocab_dim, params.embedding_dim)\n",
    "  else:\n",
    "    embedding_init = params.embedding_init\n",
    "  embedding = parameter(shape=(params.vocab_dim, params.embedding_dim), init=None)\n",
    "  embedding.value = embedding_init\n",
    "  embedding_matrix = constant(embedding_init, shape=(params.vocab_dim, params.embedding_dim))\n",
    "\n",
    "  if params.dropout_rate is not None:\n",
    "    query_embedding  = ops.dropout(times(query_sequence , embedding), params.dropout_rate, name='query_embedding')\n",
    "    context_embedding = ops.dropout(times(context_sequence, embedding), params.dropout_rate, name='context_embedding')\n",
    "  else:\n",
    "    query_embedding  = times(query_sequence , embedding, name='query_embedding')\n",
    "    context_embedding = times(context_sequence, embedding, name='context_embedding')\n",
    "\n",
    "  contextGruW = Parameter(_INFERRED +  _as_tuple(params.hidden_dim), init=glorot_uniform(), name='gru_params')\n",
    "  queryGruW = Parameter(_INFERRED +  _as_tuple(params.hidden_dim), init=glorot_uniform(), name='gru_params')\n",
    "\n",
    "  entity_embedding = ops.times(context_sequence, embedding_matrix, name='constant_entity_embedding')\n",
    "  # Unlike other words in the context, we keep the entity vectors fixed as a random vector so that each vector just means an identifier of different entities in the context and it has no semantic meaning\n",
    "  full_context_embedding = ops.element_select(entity_ids_mask, entity_embedding, context_embedding)\n",
    "  context_memory = ops.optimized_rnnstack(full_context_embedding, contextGruW, params.hidden_dim, 1, True, recurrent_op='gru', name='context_mem')\n",
    "\n",
    "  query_memory = ops.optimized_rnnstack(query_embedding, queryGruW, params.hidden_dim, 1, True, recurrent_op='gru', name='query_mem')\n",
    "  qfwd = ops.slice(sequence.last(query_memory), -1, 0, params.hidden_dim, name='fwd')\n",
    "  qbwd = ops.slice(sequence.first(query_memory), -1, params.hidden_dim, params.hidden_dim*2, name='bwd')\n",
    "  init_status = ops.splice(qfwd, qbwd, name='Init_Status') # get last fwd status and first bwd status\n",
    "  return attention_model(context_memory, query_memory, init_status, params.hidden_dim, params.attention_dim, max_steps = params.max_rl_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss fucntion\n",
    "#### Contractive Reward\n",
    "\n",
    "In the ReasoNet paper, it gives the fomular of the Reward as\n",
    "\\begin{align}\n",
    "J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\sum_{t=1}^Tr_t\\right]\n",
    "\\end{align}\n",
    "\n",
    "And it applies REINFORCE algorithm to estimate \n",
    "\\begin{align} \n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\nabla_{\\theta}log_{\\pi}\\left(t_{1:T},a_T;\\theta\\right)r_T\\right]=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b_T\\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "However, as the baseline $\\left\\{b_T;T=1...T_{max}\\right\\}$ are global variables independent of instances, it leads to slow convergence in training ReasoNet. Instead, the paper rewrite the formular as,\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) =\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)\\right]\n",
    "$$\n",
    ",where $b=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)r_T$ is the average reward on the $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "\n",
    "Since the sum of the rewards over $\\left|\\mathbb{A}^+\\right|$ episodes is zero, $\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)=0$, they call it Contractive Reward. Further more, they found using $\\left(\\frac{r_T}{b}-1\\right)$ in replace of $\\left(r_T-b\\right)$ will lead to a better convergence.\n",
    "\n",
    "In our implementation, we take the reward in the form,\n",
    "$$\n",
    "J(\\theta)=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(\\frac{r_T}{b}-1\\right) + b\n",
    "$$\n",
    "As we only compute gradient on $\\pi\\left(t_{1:T},a_T;\\theta\\right)$ and treat other components in the formula as a constant, the derivate is the same as the paper while the output is the average rewards in $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "In CNTK, we use stop_gradient operator over the output of a function to conver it to a constant in the math formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contractive_reward(labels, predictions_and_stop_probabilities):\n",
    "  \"\"\"\n",
    "  Compute the contractive reward loss in paper 'ReasoNet: Learning to Stop Reading in Machine Comprehension'\n",
    "  Args:\n",
    "    labels: The lables\n",
    "    predictions_and_stop_probabilities: A list of tuples, each tuple contains the prediction and stop probability of the coresponding step.\n",
    "  \"\"\"\n",
    "  base = None\n",
    "  avg_rewards = None\n",
    "  for step in range(len(predictions_and_stop_probabilities)):\n",
    "    pred = predictions_and_stop_probabilities[step][0]\n",
    "    stop = predictions_and_stop_probabilities[step][1]\n",
    "    if base is None:\n",
    "      base = ops.element_times(pred, stop)\n",
    "    else:\n",
    "      base = ops.plus(ops.element_times(pred, stop), base)\n",
    "  avg_rewards = ops.stop_gradient(sequence.reduce_sum(base*labels))\n",
    "  base_reward = sequence.broadcast_as(avg_rewards, base, name = 'base_line')\n",
    "  # While  the learner will mimize the loss by default, we want it to maxiumize the rewards\n",
    "  # Maxium rewards => minimal -rewards\n",
    "  # So we use (1-r/b) as the rewards instead of (r/b-1)\n",
    "  step_cr = ops.stop_gradient(1- ops.element_divide(labels, base_reward))\n",
    "  normalized_contractive_rewards = ops.element_times(base, step_cr)\n",
    "  rewards = sequence.reduce_sum(normalized_contractive_rewards) + avg_rewards\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_func(prediction, label, name='accuracy'):\n",
    "  \"\"\"\n",
    "  Compute the accuracy of the prediction\n",
    "  \"\"\"\n",
    "  pred_max = ops.hardmax(prediction, name='pred_max')\n",
    "  norm_label = ops.equal(label, [1], name='norm_label')\n",
    "  acc = ops.times_transpose(pred_max, norm_label, name='accuracy')\n",
    "  return acc\n",
    "\n",
    "def loss(model, params:model_params):\n",
    "  \"\"\"\n",
    "  Compute the loss and accuracy of the model output\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "  context = model_args['context']\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "  entities_all = sequence.gather(entity_condition, entity_condition, name='entities_all')\n",
    "  entity_ids = input_variable(shape=(params.entity_dim), is_sparse=True, dynamic_axes=entities_all.dynamic_axes, name='entity_ids')\n",
    "  wordvocab_dim = params.vocab_dim\n",
    "  labels_raw = input_variable(shape=(1,), is_sparse=False, dynamic_axes=context.dynamic_axes, name='labels')\n",
    "  answers = sequence.scatter(sequence.gather(model.outputs[-1], entity_condition), entities_all, name='Final_Ans')\n",
    "  labels = sequence.scatter(sequence.gather(labels_raw, entity_condition), entities_all, name='EntityLabels')\n",
    "  entity_id_matrix = ops.reshape(entity_ids, params.entity_dim)\n",
    "  expand_pred = sequence.reduce_sum(element_times(answers, entity_id_matrix))\n",
    "  expand_label = ops.greater_equal(sequence.reduce_sum(element_times(labels, entity_id_matrix)), 1)\n",
    "  expand_candidate_mask = ops.greater_equal(sequence.reduce_sum(entity_id_matrix), 1)\n",
    "  predictions_and_stop_probabilities=[]\n",
    "  for step in range(int((len(model.outputs)-1)/2)):\n",
    "    predictions_and_stop_probabilities += [(model.outputs[step*2], model.outputs[step*2+1])]\n",
    "  loss_value = contractive_reward(labels_raw, predictions_and_stop_probabilities)\n",
    "  accuracy = accuracy_func(expand_pred, expand_label, name='accuracy')\n",
    "  apply_loss = combine([loss_value, answers, labels, accuracy], name='Loss')\n",
    "  return apply_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_adam_learner(learn_params, learning_rate = 0.0005, gradient_clipping_threshold_per_sample=0.001):\n",
    "  \"\"\"\n",
    "  Create adam learner\n",
    "  \"\"\"\n",
    "  lr_schedule = learner.learning_rate_schedule(learning_rate, learner.UnitType.sample)\n",
    "  momentum = learner.momentum_schedule(0.90)\n",
    "  gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n",
    "  gradient_clipping_with_truncation = True\n",
    "  momentum_var = learner.momentum_schedule(0.999)\n",
    "  lr = learner.adam_sgd(learn_params, lr_schedule, momentum, True, momentum_var,\n",
    "          low_memory = False,\n",
    "          gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample,\n",
    "          gradient_clipping_with_truncation = gradient_clipping_with_truncation)\n",
    "  learner_desc = 'Alg: Adam, learning rage: {0}, momentum: {1}, gradient clip: {2}'.format(learning_rate, momentum[0], gradient_clipping_threshold_per_sample)\n",
    "  logger.log(\"Create learner. {0}\".format(learner_desc))\n",
    "  return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __evaluation(trainer, data, bind, minibatch_size, epoch_size):\n",
    "  \"\"\"\n",
    "  Evaluate the loss and accurate of the evaluation data set during training stage\n",
    "  \"\"\"\n",
    "  if epoch_size is None:\n",
    "    epoch_size = 1\n",
    "  for key in bind.keys():\n",
    "    if key.name == 'labels':\n",
    "      label_arg = key\n",
    "      break\n",
    "  eval_acc = 0\n",
    "  eval_s = 0\n",
    "  k = 0\n",
    "  print(\"Start evaluation with {0} samples ...\".format(epoch_size))\n",
    "  while k < epoch_size:\n",
    "    mbs = min(epoch_size - k, minibatch_size)\n",
    "    mb = data.next_minibatch(mbs, input_map=bind)\n",
    "    k += mb[label_arg].num_samples\n",
    "    sm = mb[label_arg].num_sequences\n",
    "    avg_acc = trainer.test_minibatch(mb)\n",
    "    eval_acc += sm*avg_acc\n",
    "    eval_s += sm\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  eval_acc /= eval_s\n",
    "  print(\"\")\n",
    "  logger.log(\"Evaluation Acc: {0}, samples: {1}\".format(eval_acc, eval_s))\n",
    "  return eval_acc\n",
    "\n",
    "def train(model, m_params:model_params, learner, train_data, max_epochs=1, save_model_flag=False, epoch_size=270000, eval_data=None, eval_size=None, check_point_freq=0.1, minibatch_size=50000, model_name='rsn'):\n",
    "  \"\"\"\n",
    "  Train the model\n",
    "  Args:\n",
    "    model: The created model\n",
    "    m_params: Model parameters\n",
    "    learner: The learner used to train the model\n",
    "  \"\"\"\n",
    "  criterion_loss = loss(model, m_params)\n",
    "  loss_func = criterion_loss.outputs[0]\n",
    "  eval_func = criterion_loss.outputs[-1]\n",
    "  trainer = Trainer(model.outputs[-1], (loss_func, eval_func), learner)\n",
    "  # Get minibatches of sequences to train with and perform model training\n",
    "  # bind inputs to data from readers\n",
    "  train_bind = bind_data(criterion_loss, train_data)\n",
    "  for k in train_bind.keys():\n",
    "    if k.name == 'labels':\n",
    "      label_key = k\n",
    "      break\n",
    "  eval_bind = bind_data(criterion_loss, eval_data)\n",
    "\n",
    "  i = 0\n",
    "  minibatch_count = 0\n",
    "  training_progress_output_freq = 500\n",
    "  check_point_interval = int(epoch_size*check_point_freq)\n",
    "  check_point_id = 0\n",
    "  for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_samples = 0\n",
    "    i = 0\n",
    "    win_loss = 0\n",
    "    win_acc = 0\n",
    "    win_samples = 0\n",
    "    chk_loss = 0\n",
    "    chk_acc = 0\n",
    "    chk_samples = 0\n",
    "    while i < epoch_size:\n",
    "      # get next minibatch of training data\n",
    "      mbs = min(minibatch_size, epoch_size - i)\n",
    "      mb_train = train_data.next_minibatch(minibatch_size, input_map=train_bind)\n",
    "      i += mb_train[label_key].num_samples\n",
    "      trainer.train_minibatch(mb_train)\n",
    "      minibatch_count += 1\n",
    "      sys.stdout.write('.')\n",
    "      sys.stdout.flush()\n",
    "      # collect epoch-wide stats\n",
    "      samples = trainer.previous_minibatch_sample_count\n",
    "      ls = trainer.previous_minibatch_loss_average * samples\n",
    "      acc = trainer.previous_minibatch_evaluation_average * samples\n",
    "      epoch_loss += ls\n",
    "      epoch_acc += acc\n",
    "      win_loss += ls\n",
    "      win_acc += acc\n",
    "      chk_loss += ls\n",
    "      chk_acc += acc\n",
    "      epoch_samples += samples\n",
    "      win_samples += samples\n",
    "      chk_samples += samples\n",
    "      if int(epoch_samples/training_progress_output_freq) != int((epoch_samples-samples)/training_progress_output_freq):\n",
    "        print('')\n",
    "        logger.log(\"Lastest sample count = {}, Train Loss: {}, Evalualtion ACC: {}\".format(win_samples, win_loss/win_samples,\n",
    "          win_acc/win_samples))\n",
    "        logger.log(\"Total sample count = {}, Train Loss: {}, Evalualtion ACC: {}\".format(chk_samples, chk_loss/chk_samples,\n",
    "          chk_acc/chk_samples))\n",
    "        win_samples = 0\n",
    "        win_loss = 0\n",
    "        win_acc = 0\n",
    "      new_chk_id = int(i/check_point_interval)\n",
    "      if new_chk_id != check_point_id and i < epoch_size :\n",
    "        check_point_id = new_chk_id\n",
    "        print('')\n",
    "        logger.log(\"--- CHECKPOINT %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (check_point_id, chk_samples, chk_loss/chk_samples, 100.0*(chk_acc/chk_samples)))\n",
    "        if eval_data:\n",
    "          __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "        if save_model_flag:\n",
    "          # save the model every epoch\n",
    "          model_filename = os.path.join('model', \"model_%s_%03d.dnn\" % (model_name, check_point_id))\n",
    "          model.save_model(model_filename)\n",
    "          logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "        chk_samples = 0\n",
    "        chk_loss = 0\n",
    "        chk_acc = 0\n",
    "\n",
    "    print('')\n",
    "    logger.log(\"--- EPOCH %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (epoch, epoch_samples, epoch_loss/epoch_samples, 100.0*(epoch_acc/epoch_samples)))\n",
    "  eval_acc = 0\n",
    "  if eval_data:\n",
    "    eval_acc = __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "  if save_model_flag:\n",
    "    # save the model every epoch\n",
    "    model_filename = os.path.join('model', \"model_%s_final.dnn\" % (model_name))\n",
    "    model.save_model(model_filename)\n",
    "    logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "  return (epoch_loss/epoch_samples, epoch_acc/epoch_samples, eval_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "from cntk.ops.tests.ops_test_utils import cntk_device\n",
    "from cntk.ops import input_variable, past_value, future_value\n",
    "from cntk.io import MinibatchSource\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "import cntk.ops as ops\n",
    "import cntk\n",
    "import math\n",
    "\n",
    "def test_reasonet():\n",
    "  data_path = train_ctf\n",
    "  eval_path = validation_ctf\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  epoch_size=289716292\n",
    "  eval_size=2993016\n",
    "  hidden_dim=384\n",
    "  max_rl_iter=5\n",
    "  max_epochs=5\n",
    "  embedding_dim=100\n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim, embedding_dim = embedding_dim, embedding_init = None, dropout_rate = 0.2)\n",
    "\n",
    "  train_data = create_reader(data_path, vocab_dim, entity_dim, True, rand_size=epoch_size)\n",
    "  eval_data = create_reader(eval_path, vocab_dim, entity_dim, False, rand_size=eval_size) if eval_path is not None else None\n",
    "  embedding_init = None\n",
    "\n",
    "  model = create_model(params)\n",
    "  learner = create_adam_learner(model.parameters)\n",
    "  (train_loss, train_acc, eval_acc) = train(model, params, learner, train_data, max_epochs=max_epochs, epoch_size=epoch_size, save_model_flag=False, model_name=os.path.basename(data_path), eval_data=eval_data, eval_size=eval_size, check_point_freq=0.1, minibatch_size = 50000)\n",
    "\n",
    "test_reasonet()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
